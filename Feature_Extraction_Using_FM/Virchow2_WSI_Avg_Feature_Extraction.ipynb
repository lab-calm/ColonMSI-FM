{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this appraoch we averaged all the pacthes features of each WSI and save WSI level averaged feature vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "from os.path import join as j_\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# print(torch.version)\n",
    "# print(torch.version.cuda)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from torchvision.transforms import Lambda\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from timm.layers import SwiGLUPacked\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "BATCH_SIZE = 1 # load each slide all tiles sequentially \n",
    "K_FOLDS_PATH = r\"E:\\KSA Project\\\\dataset\\\\splits\\kfolds_IDARS.csv\"\n",
    "DATA_PATH = r\"E:\\\\KSA Project\\\\dataset\\\\testing\\\\Patches\"\n",
    "FEATURES_SAVE_DIR = r\"E:\\\\KSA Project\\\\dataset\\\\testing\\\\virchow2_features\"\n",
    "# torch.tensor([1.2, 3.4]).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface login \n",
    "# from huggingface_hub import login\n",
    "# login()\n",
    "# need to specify MLP layer and activation function for proper init\n",
    "model = timm.create_model(\"hf-hub:paige-ai/Virchow2\", pretrained=True, mlp_layer=SwiGLUPacked, act_layer=torch.nn.SiLU)\n",
    "model_transforms = create_transform(**resolve_data_config(model.pretrained_cfg, model=model))\n",
    "\n",
    "_ = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import PatchLoader, SlideBatchSampler\n",
    "mode = 1 # for sequentially data/patches loading we will use mode =1 and mode= 2 for random loading.\n",
    "transform = transforms.Compose([\n",
    "    transforms.FiveCrop(224),  # this is a list of 5 crops\n",
    "    Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))  # convert to tensor and stack\n",
    "])\n",
    "\n",
    "def create_dataloader(label_file, data_path, transform, num_samples, mode):\n",
    "    # Create the dataset\n",
    "    dataset = PatchLoader(label_file=label_file, data_path=data_path, transform=transform, num_samples=num_samples, mode=mode)\n",
    "    # Ensure sequential data loading by disabling shuffle\n",
    "    batch_sampler = SlideBatchSampler(dataset.ntiles)\n",
    "    dataloader = DataLoader(dataset, batch_sampler=batch_sampler, num_workers=0, pin_memory=False)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "data_loader = create_dataloader(label_file=K_FOLDS_PATH,data_path=DATA_PATH,\n",
    "                                     transform=transform,num_samples=None,mode=mode)\n",
    "print(f\"Length of data_loader: {len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how can i print and view before dataloader input dataset details using \n",
    "for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "    print(f\"Batch {batch_idx+1}\")\n",
    "    print(f\"Images shape: {images.shape}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    if batch_idx == 5:  # Only print a few batches to check if it's working\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Feature Extraction on FiveCrop Patches Using Averaging Approach at WSI Level\n",
    "Extract patches of individual patch five crops then average and at the end average features of all patches of each WSI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "def extract_embeddings_with_chunks(model, transforms, dataloader, chunk_size=1000):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    device = next(model.parameters()).device\n",
    "    print(f'The size of input dataloader is {len(dataloader)}')\n",
    "\n",
    "    for batch_idx, (images, labels) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        batch_size, num_crops, channels, height, width = images.shape\n",
    "        images = images.view(batch_size * num_crops, channels, height, width)\n",
    "        images = images.permute(0, 2, 3, 1).cpu().numpy()  # Convert to (H, W, C) format\n",
    "        images = (images * 255).astype(np.uint8)  # Convert to uint8\n",
    "\n",
    "        # Preprocess images using the provided transforms\n",
    "        images = torch.stack([transforms(Image.fromarray(image)) for image in images])\n",
    "        images = images.to(device)\n",
    "\n",
    "        wsi_embeddings = []\n",
    "        num_patches = images.size(0)\n",
    "        num_chunks = (num_patches + chunk_size - 1) // chunk_size\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min(start_idx + chunk_size, num_patches)\n",
    "            chunk = images[start_idx:end_idx]\n",
    "            with torch.inference_mode(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                # Extract embeddings for the chunk\n",
    "                output = model(chunk).detach().cpu()\n",
    "                # class_token = output[:, 0]  # Extract class token\n",
    "                patch_tokens = output[:, 5:]  # Extract patch tokens, ignoring the first 4 register tokens\n",
    "                # chunk_embeddings = torch.cat([class_token, patch_tokens.mean(dim=1)], dim=-1)  # Concatenate class token and average pool of patch tokens\n",
    "                chunk_embeddings = patch_tokens.mean(dim=1)  # Average pool of patch tokens\n",
    "                # Reshape to [chunk_size, num_crops, embedding_dim]\n",
    "                chunk_embeddings = chunk_embeddings.view(-1, num_crops, chunk_embeddings.size(-1))\n",
    "                # Mean across the 5 crops\n",
    "                chunk_embeddings = chunk_embeddings.mean(dim=1)  # [chunk_size, embedding_dim]\n",
    "                # chunk_embeddings = chunk_embeddings.to(torch.float16)\n",
    "                wsi_embeddings.append(chunk_embeddings.cpu())\n",
    "\n",
    "        # Concatenate embeddings for all chunks in this WSI\n",
    "        wsi_embeddings = torch.cat(wsi_embeddings, dim=0)  # [num_patches, embedding_dim]\n",
    "        # Mean across all patches in the WSI\n",
    "        slide_embedding = wsi_embeddings.mean(dim=0)  # [embedding_dim]\n",
    "        # Take one label for the WSI\n",
    "        wsi_label = labels[0].item()\n",
    "\n",
    "        all_embeddings.append(slide_embedding.numpy())\n",
    "        all_labels.append(wsi_label)\n",
    "\n",
    "    # Stack the embeddings and labels\n",
    "    asset_dict = {\n",
    "        \"embeddings\": np.stack(all_embeddings).astype(np.float32),  # [num_slides, embedding_dim]\n",
    "        \"labels\": np.array(all_labels),\n",
    "    }\n",
    "\n",
    "    return asset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "features = extract_embeddings_with_chunks(model, model_transforms, data_loader)\n",
    "# convert these to torch\n",
    "feats = torch.Tensor(features['embeddings'])\n",
    "labels = torch.Tensor(features['labels']).type(torch.long)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f'Took {elapsed:.03f} seconds')\n",
    "print(f'Train features shape {feats.shape} and Labels shape is {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(features, dataloader, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    slides = dataloader.dataset.slides\n",
    "    for i, slide in enumerate(slides):\n",
    "        slide_feats = features[i]\n",
    "        slide_name = slide.split('.')[0]\n",
    "        # save as torch .pt file \n",
    "        save_path = os.path.join(save_dir, f'{slide_name}.pt')\n",
    "        torch.save(slide_feats, save_path)\n",
    "        # print(f'Saved features for slide {slide_name} to {save_path}')\n",
    "\n",
    "save_features(feats, data_loader, FEATURES_SAVE_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
