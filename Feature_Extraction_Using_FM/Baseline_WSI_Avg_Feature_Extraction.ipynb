{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "from os.path import join as j_\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# print(torch.version)\n",
    "# print(torch.version.cuda)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from torchvision.transforms import Lambda\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from timm.layers import SwiGLUPacked\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model \"create_model_from_pretrained\"\n",
    "By default, the model preprocessor uses 448 x 448 as the input size. To specify a different image size (e.g. 336 x 336), use the **force_img_size** argument.\n",
    "\n",
    "You can specify a cuda device by using the **device** argument, or manually move the model to a device later using **model.to(device)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "BATCH_SIZE = 1 # load each slide all tiles sequentially \n",
    "K_FOLDS_PATH = r\"E:\\Aamir Gulzar\\dataset\\splits\\kfolds_IDARS_mini.csv\"\n",
    "DATA_PATH = r\"E:\\\\Aamir Gulzar\\\\dataset\\\\Patches\"\n",
    "FEATURES_SAVE_DIR = r\"E:/Aamir Gulzar/dataset/Baseline_FiveCrop_Features\"\n",
    "# saved model path\n",
    "Model_PATH = r\"E:\\Aamir Gulzar\\existing_approaches\\Baseline_Fivecrop_4Folds\\MSI_vs_MSS\\fold1\\best0\\checkpoint_best_AUC.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet34()\n",
    "model.fc = nn.Identity()\n",
    "checkpoint = torch.load(Model_PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "def preprocess_resnet(image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Ensure consistent size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))  # ImageNet normalization\n",
    "    ])\n",
    "    return transform(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchLoader(Dataset):\n",
    "    def __init__(self, label_file, data_path, transform=None, num_samples=None, img_resize=False, mode=2):     \n",
    "        lib = pd.DataFrame(pd.read_csv(label_file, usecols=['WSI_Id', 'label_id'], keep_default_na=True))\n",
    "        lib.dropna(inplace=True)\n",
    "        if num_samples is not None:\n",
    "            lib = lib.sample(n=num_samples)\n",
    "        tar = lib['label_id'].values.tolist()\n",
    "        allslides = lib['WSI_Id'].values.tolist()       \n",
    "        slides = []\n",
    "        tiles = []\n",
    "        ntiles = []\n",
    "        slideIDX = []\n",
    "        targets = []\n",
    "        j = 0\n",
    "        for i, path in enumerate(allslides):\n",
    "            t = []\n",
    "            cpath = os.path.join(data_path, str(path))\n",
    "            if not os.path.exists(cpath):\n",
    "                # print('This slide does not exist: {}'.format(path))\n",
    "                continue\n",
    "            else:\n",
    "                # print('This slide exists: {}'.format(path))\n",
    "                # count = 0\n",
    "                for f in os.listdir(cpath): \n",
    "                    if '.png' in f:\n",
    "                        # count = count + 1\n",
    "                        t.append(os.path.join(cpath, f))\n",
    "                if len(t) > 0:\n",
    "                    slides.append(path)\n",
    "                    tiles.extend(t)\n",
    "                    ntiles.append(len(t))\n",
    "                    slideIDX.extend([j]*len(t))\n",
    "                    targets.append(int(tar[i]))\n",
    "                    j+=1\n",
    "        print('Number of Slides: {}'.format(len(slides)))\n",
    "        print('Number of tiles: {}'.format(len(tiles)))\n",
    "        self.slides = slides\n",
    "        self.slideIDX = slideIDX\n",
    "        self.ntiles = ntiles\n",
    "        self.tiles = tiles\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        self.img_resize = img_resize\n",
    "        self.mode = mode\n",
    "\n",
    "    def maketraindata(self, idxs):\n",
    "        self.t_data = [(self.slideIDX[x], self.tiles[x], self.targets[self.slideIDX[x]]) for x in idxs]\n",
    "\n",
    "    def shuffletraindata(self):\n",
    "        self.t_data = random.sample(self.t_data, len(self.t_data))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 1:# loads all tiles from each slide sequentially for train/validatoin set\n",
    "            tile = self.tiles[index]\n",
    "            img = Image.open(str(tile)).convert('RGB')\n",
    "            slideIDX = self.slideIDX[index]\n",
    "            target = self.targets[slideIDX]\n",
    "            if self.img_resize== True:  \n",
    "                img = img.resize((224, 224), Image.BILINEAR)\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            return img, target\n",
    "        elif self.mode == 2:  # used when a different trainset is prepared e.g. with given tile index    \n",
    "            slideIDX, tile, target = self.t_data[index]\n",
    "            img = Image.open(str(tile)).convert('RGB')\n",
    "            if self.img_resize == True:\n",
    "                img = img.resize((224, 224), Image.BILINEAR)\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 1:\n",
    "            length = len(self.tiles)\n",
    "        elif self.mode == 2:\n",
    "            length = len(self.t_data)\n",
    "        else:\n",
    "            length = 0\n",
    "        # print(f\"__len__ called, mode: {self.mode}, length: {length}\")\n",
    "        return length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Slides: 7\n",
      "Number of tiles: 5324\n",
      "Length of train_loader: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:45: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:45: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\DataInsight GPU\\AppData\\Local\\Temp\\ipykernel_203512\\1865940351.py:45: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  train_dataloader = create_dataloader(label_file=\"E:\\Aamir Gulzar\\dataset\\splits\\kfolds_IDARS_mini.csv\",data_path=\"E:\\\\Aamir Gulzar\\\\dataset\\\\Patches\",\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Sampler, DataLoader\n",
    "mode = 1 # for sequentially data/patches loading we will use mode =1 and mode= 2 for random loading.\n",
    "class SlideBatchSampler(Sampler):\n",
    "    def __init__(self, ntiles):\n",
    "        # ntiles contains the number of tiles per slide\n",
    "        self.ntiles = ntiles\n",
    "        self.indices = []\n",
    "        start_idx = 0\n",
    "        for num_tiles in ntiles:\n",
    "            self.indices.append(list(range(start_idx, start_idx + num_tiles)))\n",
    "            start_idx += num_tiles\n",
    "    def __iter__(self):\n",
    "        # Yield each set of indices for a single slide (batch contains all tiles for that slide)\n",
    "        for batch in self.indices:\n",
    "            yield batch\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.FiveCrop(224),  # this is a list of 5 crops\n",
    "    Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))  # convert to tensor and stack\n",
    "])\n",
    "\n",
    "# Custom collate function to handle the batch of crops\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    batch_size, num_crops, c, h, w = images.size()\n",
    "    images = images.view(-1, c, h, w)  # flatten the crops into individual images\n",
    "    labels = torch.tensor(labels).repeat_interleave(num_crops)  # repeat labels for each crop\n",
    "    return images, labels\n",
    "\n",
    "def create_dataloader(label_file, data_path, transform, num_samples,shuffle, img_resize, mode):\n",
    "    dataset = PatchLoader(label_file=label_file, data_path=data_path, transform=transform, num_samples=num_samples, img_resize=img_resize, mode=mode)\n",
    "    num_tiles = len(dataset.slideIDX)\n",
    "    dataset.maketraindata(np.arange(num_tiles))\n",
    "    if shuffle:\n",
    "        dataset.shuffletraindata()\n",
    "        # Use SlideBatchSampler instead of a fixed batch size\n",
    "    batch_sampler = SlideBatchSampler(dataset.ntiles)\n",
    "    dataloader = DataLoader(dataset, batch_sampler=batch_sampler, num_workers=0, pin_memory=False)\n",
    "    return dataloader\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = create_dataloader(label_file=\"E:\\Aamir Gulzar\\dataset\\splits\\kfolds_IDARS_mini.csv\",data_path=\"E:\\\\Aamir Gulzar\\\\dataset\\\\Patches\",\n",
    "                                     transform=transform,num_samples=7,shuffle=True,img_resize=False,mode=mode)\n",
    "print(f\"Length of train_loader: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Images shape: torch.Size([932, 5, 3, 224, 224])\n",
      "Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Batch 2\n",
      "Images shape: torch.Size([1359, 5, 3, 224, 224])\n",
      "Labels: tensor([1, 1, 1,  ..., 1, 1, 1])\n",
      "Batch 3\n",
      "Images shape: torch.Size([580, 5, 3, 224, 224])\n",
      "Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (images, labels) in enumerate(train_dataloader):\n",
    "    print(f\"Batch {batch_idx+1}\")\n",
    "    print(f\"Images shape: {images.shape}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    if batch_idx == 2:  # Only print a few batches to check if it's working\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Feature Extraction on FiveCrop Patches Using Averaging Approach at WSI Level\n",
    "Extract patches of individual patch five crops then average and at the end average features of all patches of each WSI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_embeddings_with_chunks(model, preprocess, dataloader, chunk_size=1000):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    device = next(model.parameters()).device\n",
    "    # print(f'The size of input dataloader is {len(dataloader)}')\n",
    "\n",
    "    for batch_idx, (images, labels) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        batch_size, num_crops, channels, height, width = images.shape\n",
    "        images = images.view(batch_size * num_crops, channels, height, width)\n",
    "        images = images.permute(0, 2, 3, 1).cpu().numpy()  # Convert to (H, W, C) format\n",
    "        images = (images * 255).astype(np.uint8)  # Convert to uint8\n",
    "\n",
    "        # Preprocess images\n",
    "        images = torch.stack([preprocess(Image.fromarray(image)) for image in images])\n",
    "        images = images.to(device)\n",
    "\n",
    "        wsi_embeddings = []\n",
    "        num_patches = images.size(0)\n",
    "        num_chunks = (num_patches + chunk_size - 1) // chunk_size\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min(start_idx + chunk_size, num_patches)\n",
    "            chunk = images[start_idx:end_idx]\n",
    "            with torch.inference_mode():\n",
    "                # Extract embeddings for the chunk\n",
    "                chunk_embeddings = model(chunk).detach().cpu()\n",
    "                chunk_embeddings = chunk_embeddings.view(-1, num_crops, chunk_embeddings.size(-1))  # Reshape to [chunk_size, num_crops, embedding_dim]\n",
    "                chunk_embeddings = chunk_embeddings.mean(dim=1)  # Average over five-crop features\n",
    "                wsi_embeddings.append(chunk_embeddings)\n",
    "        # Concatenate embeddings for all chunks in this WSI\n",
    "        wsi_embeddings = torch.cat(wsi_embeddings, dim=0)  # [num_patches, embedding_dim]\n",
    "        # print(f'WSI embeddings shape: {wsi_embeddings.shape}')\n",
    "        slide_embedding = wsi_embeddings.mean(dim=0)  # Average over all patches\n",
    "        wsi_label = labels[0].item()  # Assuming all labels in the batch are the same\n",
    "        all_embeddings.append(slide_embedding.numpy())\n",
    "        all_labels.append(wsi_label)\n",
    "        # print (f'Batch {batch_idx} done')\n",
    "        # print(f'shape of all embeddings: {len(all_embeddings)}')\n",
    "\n",
    "    # Stack the embeddings and labels\n",
    "    asset_dict = {\n",
    "        \"embeddings\": np.stack(all_embeddings).astype(np.float32),  # [num_slides, embedding_dim]\n",
    "        \"labels\": np.array(all_labels),\n",
    "    }\n",
    "\n",
    "    return asset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:12<00:00, 18.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 132.856 seconds\n",
      "Train features shape torch.Size([7, 512]) and Labels shape is torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_features = extract_embeddings_with_chunks(model, preprocess_resnet, train_dataloader)\n",
    "# convert these to torch\n",
    "train_feats = torch.Tensor(train_features['embeddings'])\n",
    "train_labels = torch.Tensor(train_features['labels']).type(torch.long)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f'Took {elapsed:.03f} seconds')\n",
    "print(f'Train features shape {train_feats.shape} and Labels shape is {train_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now i wwanted to save these extracted features and nam each file with the slide name that can be get from train_dataloader.dataset.slides against each feature in train_feats\n",
    "# i will use the same for val_feats and test_feats\n",
    "# save the extracted features\n",
    "\n",
    "# Define directories for train, validation, and test splits\n",
    "train_dir = r\"E:\\Aamir Gulzar\\dataset\\\\baseline_new_avg_features\"\n",
    "\n",
    "def save_features(features, dataloader, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    slides = dataloader.dataset.slides\n",
    "    for i, slide in enumerate(slides):\n",
    "        slide_feats = features[i]\n",
    "        slide_name = slide.split('.')[0]\n",
    "        # save as torch .pt file \n",
    "        save_path = os.path.join(save_dir, f'{slide_name}.pt')\n",
    "        torch.save(slide_feats, save_path)\n",
    "        # print(f'Saved features for slide {slide_name} to {save_path}')\n",
    "\n",
    "save_features(train_feats, train_dataloader, train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
