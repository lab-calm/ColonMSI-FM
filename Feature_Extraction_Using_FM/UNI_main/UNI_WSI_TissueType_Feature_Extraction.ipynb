{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "from os.path import join as j_\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# print(torch.version)\n",
    "# print(torch.version.cuda)\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Lambda\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "# loading all packages here to start\n",
    "from uni import get_encoder\n",
    "from uni.downstream.eval_patch_features.linear_probe import eval_linear_probe\n",
    "from uni.downstream.eval_patch_features.fewshot import eval_knn, eval_fewshot\n",
    "from uni.downstream.eval_patch_features.protonet import ProtoNet, prototype_topk_vote\n",
    "from uni.downstream.eval_patch_features.metrics import get_eval_metrics, print_metrics\n",
    "from uni.downstream.utils import concat_images\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "# Define directories for train, validation, and test splits\n",
    "train =  torch.load(r\"E:\\KSA Project\\dataset\\uni_fivecrop_features\\train\\TCGA-3L-AA1B_nonMSIH\\TCGA-3L-AA1B_nonMSIH_0.pt\")\n",
    "valid = torch.load(r\"E:\\KSA Project\\dataset\\uni_fivecrop_features\\valid\\TCGA-A6-5661_MSIH\\TCGA-A6-5661_MSIH_0.pt\")\n",
    "test = torch.load(r\"E:\\KSA Project\\dataset\\uni_fivecrop_features\\test\\TCGA-A6-2686_MSIH\\TCGA-A6-2686_MSIH_0.pt\")\n",
    "# print shapes \n",
    "print(train[0].shape)\n",
    "print(valid[1].shape)  \n",
    "print(test[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained tissue type linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1024, out_features=224, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.3, inplace=False)\n",
       "  (3): Linear(in_features=224, out_features=128, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Dropout(p=0.3, inplace=False)\n",
       "  (6): Linear(in_features=128, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the complete saved model and class instance\n",
    "model_save_path = r\"E:\\KSA Project\\new_feature_extraction_approach\\ann_tissue_classifier_uni.pth\"\n",
    "tissue_classifier = torch.load(model_save_path)\n",
    "# Set the model to evaluation mode if needed\n",
    "tissue_classifier.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read FiveCrop Patch Level Features and Concatenate all the same tissue type features at WSI level and save these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features_and_labels(save_dir,save_feat, device='cpu'):\n",
    "    wsi_count = 0\n",
    "    all_wsi_features = []\n",
    "    all_wsi_labels = []\n",
    "    # Loop through each WSI folder\n",
    "    for wsi_folder in os.listdir(save_dir):\n",
    "        wsi_folder_path = os.path.join(save_dir, wsi_folder)\n",
    "        if os.path.isdir(wsi_folder_path):  # Ensure it is a directory\n",
    "            \n",
    "            # Initialize lists for each tissue type (T1 to T9)\n",
    "            T0_feats, T1_feats, T2_feats, T3_feats, T4_feats, T5_feats, T6_feats, T7_feats, T8_feats = ([] for _ in range(9))\n",
    "            # Loop through each patch file (.pt) inside the WSI folder\n",
    "            patch_count = 0\n",
    "            for file_name in os.listdir(wsi_folder_path):\n",
    "                if file_name.endswith('.pt'):\n",
    "                    file_path = os.path.join(wsi_folder_path, file_name)\n",
    "                    # Load the patch features (which contain 5 crops)\n",
    "                    patch = torch.load(file_path, map_location=device)\n",
    "                    # Process each crop for the patch\n",
    "                    for i, patch_crop in enumerate(patch):\n",
    "                        patch_crop = patch_crop.unsqueeze(0)  # Add batch dimension\n",
    "                        # Simulate the tissue type classification by assigning a random type\n",
    "                        p_crop_tissue_type = torch.argmax(tissue_classifier(patch_crop),dim=1)\n",
    "                        patch_crop = patch_crop.squeeze(0)  # Add batch dimension for model input\n",
    "                        # p_crop_tissue_type = random.randint(0, 8)\n",
    "                        # Add the crop to the appropriate tissue type list\n",
    "                        if p_crop_tissue_type == 0:\n",
    "                            T0_feats.append(patch_crop)\n",
    "                        elif p_crop_tissue_type == 1:\n",
    "                            T1_feats.append(patch_crop)\n",
    "                        elif p_crop_tissue_type == 2:\n",
    "                            T2_feats.append(patch_crop)\n",
    "                        elif p_crop_tissue_type == 3:\n",
    "                            T3_feats.append(patch_crop)\n",
    "                        elif p_crop_tissue_type == 4:\n",
    "                            T4_feats.append(patch_crop)\n",
    "                        elif p_crop_tissue_type == 5:\n",
    "                            T5_feats.append(patch_crop)\n",
    "                        elif p_crop_tissue_type == 6:\n",
    "                            T6_feats.append(patch_crop)\n",
    "                        elif p_crop_tissue_type == 7:\n",
    "                            T7_feats.append(patch_crop)\n",
    "                        elif p_crop_tissue_type == 8:\n",
    "                            T8_feats.append(patch_crop)\n",
    "\n",
    "            # Function to safely average features if the list is not empty, else return a zero vector\n",
    "            def average_features(feat_list, feat_dim):\n",
    "                if feat_list:\n",
    "                    return torch.stack(feat_list).mean(dim=0).to(device)\n",
    "                else:\n",
    "                    return torch.zeros(feat_dim, device=device)  # Return a zero vector of the same feature dimension\n",
    "\n",
    "            # Assuming feature dimension from the first patch crop (modify as needed)\n",
    "            # feat_dim = patch[0].shape[0] if patch else 512  # Default to 512 if no patches\n",
    "            feat_dim = 1024\n",
    "\n",
    "            # Compute the average features for each tissue type, ensuring non-empty lists\n",
    "            T0 = average_features(T0_feats, feat_dim)\n",
    "            T1 = average_features(T1_feats, feat_dim)\n",
    "            T2 = average_features(T2_feats, feat_dim)\n",
    "            T3 = average_features(T3_feats, feat_dim)\n",
    "            T4 = average_features(T4_feats, feat_dim)\n",
    "            T5 = average_features(T5_feats, feat_dim)\n",
    "            T6 = average_features(T6_feats, feat_dim)\n",
    "            T7 = average_features(T7_feats, feat_dim)\n",
    "            T8 = average_features(T8_feats, feat_dim)\n",
    "\n",
    "            # Print the lengths of the tissue type lists for the current WSI\n",
    "            # print(f\"Lengths of features: T1={len(T0_feats)}, T2={len(T1_feats)}, T3={len(T2_feats)}, T4={len(T3_feats)}, T5={len(T4_feats)}, T6={len(T5_feats)}, T7={len(T6_feats)}, T8={len(T7_feats)}, T9={len(T8_feats)}\")\n",
    "\n",
    "            # Concatenate the feature vectors of all tissue types into a single list for the WSI\n",
    "            WSI_feature_list = torch.stack([T0, T1, T2, T3, T4, T5, T6, T7, T8])\n",
    "            # Print the length of the final feature vector for this WSI\n",
    "            # print(f\"Length of feature vector for WSI {wsi_folder}: {len(WSI_feature_list)}\")\n",
    "            # Save the WSI feature list (optional)\n",
    "            torch.save(WSI_feature_list, os.path.join(save_feat, f'{wsi_folder}.pt'))\n",
    "    \n",
    "\n",
    "# Define directories for train, validation, and test splits\n",
    "train_dir = r\"E:\\Aamir Gulzar\\dataset\\\\uni_fivecrop_features\\\\train\"\n",
    "valid_dir = r\"E:\\Aamir Gulzar\\dataset\\\\uni_fivecrop_features\\\\valid\"\n",
    "test_dir = r\"E:\\Aamir Gulzar\\dataset\\\\uni_fivecrop_features\\\\test\"\n",
    "\n",
    "save_train_feat = r\"E:\\Aamir Gulzar\\dataset\\\\uni_tissue_features\\\\train\"\n",
    "save_valid_feat = r\"E:\\Aamir Gulzar\\dataset\\\\uni_tissue_features\\\\valid\"\n",
    "save_test_feat = r\"E:\\Aamir Gulzar\\dataset\\\\uni_tissue_features\\\\test\"\n",
    "\n",
    "# Load features and labels for each split\n",
    "load_features_and_labels(train_dir,save_train_feat )\n",
    "load_features_and_labels(valid_dir, save_valid_feat)\n",
    "load_features_and_labels(test_dir, save_test_feat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Tissue Classes Saved Features and select two best performing tissue types for linear models training\n",
    "#### Load all tissue classes features of each WSI and train linear models.\n",
    "\n",
    "In this approach read all the five crop level patches of all patches and make ten lists of features of each WSI according to tissue classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload saved feature for KFolds Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\K'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\K'\n",
      "C:\\Users\\datai\\AppData\\Local\\Temp\\ipykernel_4632\\145906328.py:6: SyntaxWarning: invalid escape sequence '\\K'\n",
      "  folds_df = pd.read_csv(\"E:\\KSA Project\\dataset\\splits\\kfolds.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load the Excel file with patient IDs for each fold\n",
    "folds_df = pd.read_csv(\"E:\\KSA Project\\dataset\\splits\\kfolds.csv\")\n",
    "\n",
    "# Convert the patient IDs for each fold to a list (truncate to first 12 characters)\n",
    "fold1_ids = folds_df['Fold1'].dropna().apply(lambda x: x[:12]).tolist()\n",
    "fold2_ids = folds_df['Fold2'].dropna().apply(lambda x: x[:12]).tolist()\n",
    "fold3_ids = folds_df['Fold3'].dropna().apply(lambda x: x[:12]).tolist()\n",
    "fold4_ids = folds_df['Fold4'].dropna().apply(lambda x: x[:12]).tolist()\n",
    "\n",
    "folds = [\n",
    "    (fold1_ids, \"Fold 1\"),\n",
    "    (fold2_ids, \"Fold 2\"),\n",
    "    (fold3_ids, \"Fold 3\"),\n",
    "    (fold4_ids, \"Fold 4\")\n",
    "]\n",
    "\n",
    "def load_features_and_labels(save_dir, fold_ids):\n",
    "    all_wsi_feature_list = []\n",
    "    all_wsi_label_list = []\n",
    "    # Loop through each WSI folder\n",
    "    for wsi_file in os.listdir(save_dir):\n",
    "        wsi_path = os.path.join(save_dir, wsi_file)\n",
    "\n",
    "        # Extract the WSI ID and truncate it to the first 12 characters\n",
    "        wsi_id = wsi_file[:12]  # Extract first 12 characters\n",
    "\n",
    "        # Check if the WSI ID is in the fold's ID list\n",
    "        if wsi_id not in fold_ids:\n",
    "            continue  # Skip if the WSI is not in the current fold\n",
    "\n",
    "        if wsi_path.endswith('.pt'):\n",
    "            file_path = os.path.join(wsi_file, wsi_path)\n",
    "\n",
    "            # Load the features\n",
    "            wsi_features = torch.load(file_path)\n",
    "            # if you are interested to get results of only two best performing tissue types (MUC & TUM) of each WSI then use below line otherwise comment it to use all tissue types featureas\n",
    "            wsi_features = torch.cat([wsi_features[4], wsi_features[8]])\n",
    "            wsi_features = wsi_features.view(-1)\n",
    "\n",
    "            # Append WSI feature to the list\n",
    "            all_wsi_feature_list.append(wsi_features)\n",
    "\n",
    "            # Determine label based on WSI folder name\n",
    "            if '_nonMSI' in wsi_file:\n",
    "                all_wsi_label_list.append(0)\n",
    "            elif '_MSI' in wsi_file:\n",
    "                all_wsi_label_list.append(1)\n",
    "\n",
    "    # Stack all WSI features and labels\n",
    "    features = torch.stack(all_wsi_feature_list)\n",
    "    labels = torch.tensor(all_wsi_label_list)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, (int, float)):  # Check if the value is a number\n",
    "            print(f\"{key}: {value:.4f}\")  # Format numbers with 4 decimal places\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")  # For non-numeric values, just print them directly\n",
    "\n",
    "save_dir = r\"E:\\KSA Project\\dataset\\uni_tissue_features\\all_data\"\n",
    "\n",
    "def average_metrics(results_per_fold):\n",
    "    average_results = {}\n",
    "\n",
    "    # Iterate over each fold's result\n",
    "    for fold_result in results_per_fold:\n",
    "        for key, value in fold_result.items():\n",
    "            if isinstance(value, (int, float)):  # If it's a numeric value, sum it up\n",
    "                if key not in average_results:\n",
    "                    average_results[key] = 0\n",
    "                average_results[key] += value\n",
    "            elif isinstance(value, dict):  # If it's a dictionary, recursively handle it\n",
    "                if key not in average_results:\n",
    "                    average_results[key] = {}\n",
    "                for sub_key, sub_value in value.items():\n",
    "                    if isinstance(sub_value, (int, float)):\n",
    "                        if sub_key not in average_results[key]:\n",
    "                            average_results[key][sub_key] = 0\n",
    "                        average_results[key][sub_key] += sub_value\n",
    "\n",
    "    # Divide by the number of folds to get the average\n",
    "    num_folds = len(results_per_fold)\n",
    "    for key, value in average_results.items():\n",
    "        if isinstance(value, (int, float)):  # Average numeric values\n",
    "            average_results[key] /= num_folds\n",
    "        elif isinstance(value, dict):  # Average values in nested dictionaries\n",
    "            for sub_key, sub_value in value.items():\n",
    "                average_results[key][sub_key] /= num_folds\n",
    "\n",
    "    return average_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for Fold 1 as test set\n",
      "Training on 305 samples, Testing on 100 samples\n",
      "Linear Probe Evaluation: Train shape torch.Size([305, 2048])\n",
      "Linear Probe Evaluation: Test shape torch.Size([100, 2048])\n",
      "Linear Probe Evaluation (Train Time): Best cost = 40.960\n",
      "Linear Probe Evaluation (Train Time): Using only train set for evaluation. Train Shape:  torch.Size([305, 2048])\n",
      "(Before Training) Loss: 0.693\n",
      "(After Training) Loss: 0.036\n",
      "Linear Probe Evaluation (Test Time): Test Shape torch.Size([100, 2048])\n",
      "Confusion Matrix:\n",
      "[[80  5]\n",
      " [ 8  7]]\n",
      "Linear Probe Evaluation: Time taken 0.70\n",
      "Results for Fold 1:\n",
      "lin_acc: 0.8700\n",
      "lin_bacc: 0.7039\n",
      "lin_kappa: 0.4444\n",
      "lin_weighted_f1: 0.8639\n",
      "lin_report: {'0': {'precision': 0.9090909090909091, 'recall': 0.9411764705882353, 'f1-score': 0.9248554913294798, 'support': 85.0}, '1': {'precision': 0.5833333333333334, 'recall': 0.4666666666666667, 'f1-score': 0.5185185185185185, 'support': 15.0}, 'accuracy': 0.87, 'macro avg': {'precision': 0.7462121212121212, 'recall': 0.7039215686274509, 'f1-score': 0.7216870049239992, 'support': 100.0}, 'weighted avg': {'precision': 0.8602272727272726, 'recall': 0.87, 'f1-score': 0.8639049454078355, 'support': 100.0}}\n",
      "lin_auroc: 0.9059\n",
      "Running for Fold 2 as test set\n",
      "Training on 311 samples, Testing on 94 samples\n",
      "Linear Probe Evaluation: Train shape torch.Size([311, 2048])\n",
      "Linear Probe Evaluation: Test shape torch.Size([94, 2048])\n",
      "Linear Probe Evaluation (Train Time): Best cost = 40.960\n",
      "Linear Probe Evaluation (Train Time): Using only train set for evaluation. Train Shape:  torch.Size([311, 2048])\n",
      "(Before Training) Loss: 0.693\n",
      "(After Training) Loss: 0.036\n",
      "Linear Probe Evaluation (Test Time): Test Shape torch.Size([94, 2048])\n",
      "Confusion Matrix:\n",
      "[[79  2]\n",
      " [ 7  6]]\n",
      "Linear Probe Evaluation: Time taken 0.56\n",
      "Results for Fold 2:\n",
      "lin_acc: 0.9043\n",
      "lin_bacc: 0.7184\n",
      "lin_kappa: 0.5210\n",
      "lin_weighted_f1: 0.8943\n",
      "lin_report: {'0': {'precision': 0.9186046511627907, 'recall': 0.9753086419753086, 'f1-score': 0.9461077844311377, 'support': 81.0}, '1': {'precision': 0.75, 'recall': 0.46153846153846156, 'f1-score': 0.5714285714285714, 'support': 13.0}, 'accuracy': 0.9042553191489362, 'macro avg': {'precision': 0.8343023255813953, 'recall': 0.7184235517568851, 'f1-score': 0.7587681779298545, 'support': 94.0}, 'weighted avg': {'precision': 0.8952869866402771, 'recall': 0.9042553191489362, 'f1-score': 0.8942904464626977, 'support': 94.0}}\n",
      "lin_auroc: 0.8860\n",
      "Running for Fold 3 as test set\n",
      "Training on 294 samples, Testing on 111 samples\n",
      "Linear Probe Evaluation: Train shape torch.Size([294, 2048])\n",
      "Linear Probe Evaluation: Test shape torch.Size([111, 2048])\n",
      "Linear Probe Evaluation (Train Time): Best cost = 40.960\n",
      "Linear Probe Evaluation (Train Time): Using only train set for evaluation. Train Shape:  torch.Size([294, 2048])\n",
      "(Before Training) Loss: 0.693\n",
      "(After Training) Loss: 0.034\n",
      "Linear Probe Evaluation (Test Time): Test Shape torch.Size([111, 2048])\n",
      "Confusion Matrix:\n",
      "[[87  4]\n",
      " [ 8 12]]\n",
      "Linear Probe Evaluation: Time taken 0.58\n",
      "Results for Fold 3:\n",
      "lin_acc: 0.8919\n",
      "lin_bacc: 0.7780\n",
      "lin_kappa: 0.6031\n",
      "lin_weighted_f1: 0.8870\n",
      "lin_report: {'0': {'precision': 0.9157894736842105, 'recall': 0.9560439560439561, 'f1-score': 0.9354838709677419, 'support': 91.0}, '1': {'precision': 0.75, 'recall': 0.6, 'f1-score': 0.6666666666666666, 'support': 20.0}, 'accuracy': 0.8918918918918919, 'macro avg': {'precision': 0.8328947368421052, 'recall': 0.778021978021978, 'f1-score': 0.8010752688172043, 'support': 111.0}, 'weighted avg': {'precision': 0.8859174964438122, 'recall': 0.8918918918918919, 'f1-score': 0.8870483386612418, 'support': 111.0}}\n",
      "lin_auroc: 0.9143\n",
      "Running for Fold 4 as test set\n",
      "Training on 305 samples, Testing on 100 samples\n",
      "Linear Probe Evaluation: Train shape torch.Size([305, 2048])\n",
      "Linear Probe Evaluation: Test shape torch.Size([100, 2048])\n",
      "Linear Probe Evaluation (Train Time): Best cost = 40.960\n",
      "Linear Probe Evaluation (Train Time): Using only train set for evaluation. Train Shape:  torch.Size([305, 2048])\n",
      "(Before Training) Loss: 0.693\n",
      "(After Training) Loss: 0.033\n",
      "Linear Probe Evaluation (Test Time): Test Shape torch.Size([100, 2048])\n",
      "Confusion Matrix:\n",
      "[[83  4]\n",
      " [ 7  6]]\n",
      "Linear Probe Evaluation: Time taken 0.57\n",
      "Results for Fold 4:\n",
      "lin_acc: 0.8900\n",
      "lin_bacc: 0.7078\n",
      "lin_kappa: 0.4608\n",
      "lin_weighted_f1: 0.8838\n",
      "lin_report: {'0': {'precision': 0.9222222222222223, 'recall': 0.9540229885057471, 'f1-score': 0.9378531073446328, 'support': 87.0}, '1': {'precision': 0.6, 'recall': 0.46153846153846156, 'f1-score': 0.5217391304347826, 'support': 13.0}, 'accuracy': 0.89, 'macro avg': {'precision': 0.7611111111111111, 'recall': 0.7077807250221043, 'f1-score': 0.7297961188897077, 'support': 100.0}, 'weighted avg': {'precision': 0.8803333333333333, 'recall': 0.89, 'f1-score': 0.8837582903463523, 'support': 100.0}}\n",
      "lin_auroc: 0.7984\n",
      "\n",
      "Average Results across all folds:\n",
      "lin_acc: 0.8890\n",
      "lin_bacc: 0.7270\n",
      "lin_kappa: 0.5073\n",
      "lin_weighted_f1: 0.8823\n",
      "lin_report: {'accuracy': 0.889036802760207}\n",
      "lin_auroc: 0.8762\n"
     ]
    }
   ],
   "source": [
    "# Function to run cross-validation\n",
    "def run_k_fold_cross_validation():\n",
    "    results_per_fold = []\n",
    "\n",
    "    for i, (test_ids, fold_name) in enumerate(folds):\n",
    "        # Prepare training and validation sets (all folds except the test fold)\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for j, (train_ids, _) in enumerate(folds):\n",
    "            if i == j:\n",
    "                continue  # Skip the test fold\n",
    "            fold_features, fold_labels = load_features_and_labels(save_dir, train_ids)\n",
    "            train_features.append(fold_features)\n",
    "            train_labels.append(fold_labels)\n",
    "\n",
    "        # Concatenate training features and labels from all training folds\n",
    "        train_features = torch.cat(train_features)\n",
    "        train_labels = torch.cat(train_labels)\n",
    "\n",
    "        # Load test data (current fold as the test set)\n",
    "        test_features, test_labels = load_features_and_labels(save_dir, test_ids)\n",
    "\n",
    "        print(f\"Running for {fold_name} as test set\")\n",
    "        print(f\"Training on {train_features.shape[0]} samples, Testing on {test_features.shape[0]} samples\")\n",
    "\n",
    "        # Train and evaluate the model on the current fold\n",
    "        from uni.downstream.eval_patch_features.linear_probe import eval_linear_probe\n",
    "\n",
    "        linprobe_eval_metrics, linprobe_dump = eval_linear_probe(\n",
    "            train_feats=train_features,\n",
    "            train_labels=train_labels,\n",
    "            valid_feats=None,  # Use test set as validation for simplicity\n",
    "            valid_labels=None,\n",
    "            test_feats=test_features,\n",
    "            test_labels=test_labels,\n",
    "            max_iter=50,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        print(f\"Results for {fold_name}:\")\n",
    "        print_metrics(linprobe_eval_metrics)\n",
    "        results_per_fold.append(linprobe_eval_metrics)\n",
    "\n",
    "    return results_per_fold\n",
    "\n",
    "# Run k-fold cross-validation\n",
    "all_fold_results = run_k_fold_cross_validation()\n",
    "average_results = average_metrics(all_fold_results)\n",
    "print(\"\\nAverage Results across all folds:\")\n",
    "print_metrics(average_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for Fold 1 as test set\n",
      "Training on 305 samples, Testing on 100 samples\n",
      "ANN Probe Evaluation: Train shape torch.Size([305, 2048])\n",
      "ANN Probe Evaluation: Test shape torch.Size([100, 2048])\n",
      "ANN Probe Evaluation (Train Time): Best cost = 40.960\n",
      "ANN Probe Evaluation (Train Time): Using only train set for evaluation. Train Shape:  torch.Size([305, 2048])\n",
      "(After Training) Loss: 27.647\n",
      "ANN Probe Evaluation (Test Time): Test Shape torch.Size([100, 2048])\n",
      "Confusion Matrix:\n",
      "[[68 17]\n",
      " [ 2 13]]\n",
      "ANN Probe Evaluation: Time taken 0.86\n",
      "Results for Fold 1:\n",
      "ann_acc: 0.8100\n",
      "ann_bacc: 0.8333\n",
      "ann_kappa: 0.4722\n",
      "ann_weighted_f1: 0.8325\n",
      "ann_report: {'0': {'precision': 0.9714285714285714, 'recall': 0.8, 'f1-score': 0.8774193548387097, 'support': 85.0}, '1': {'precision': 0.43333333333333335, 'recall': 0.8666666666666667, 'f1-score': 0.5777777777777777, 'support': 15.0}, 'accuracy': 0.81, 'macro avg': {'precision': 0.7023809523809523, 'recall': 0.8333333333333334, 'f1-score': 0.7275985663082437, 'support': 100.0}, 'weighted avg': {'precision': 0.8907142857142857, 'recall': 0.81, 'f1-score': 0.8324731182795699, 'support': 100.0}}\n",
      "ann_auroc: 0.8949\n",
      "Running for Fold 2 as test set\n",
      "Training on 311 samples, Testing on 94 samples\n",
      "ANN Probe Evaluation: Train shape torch.Size([311, 2048])\n",
      "ANN Probe Evaluation: Test shape torch.Size([94, 2048])\n",
      "ANN Probe Evaluation (Train Time): Best cost = 40.960\n",
      "ANN Probe Evaluation (Train Time): Using only train set for evaluation. Train Shape:  torch.Size([311, 2048])\n",
      "(After Training) Loss: 27.620\n",
      "ANN Probe Evaluation (Test Time): Test Shape torch.Size([94, 2048])\n",
      "Confusion Matrix:\n",
      "[[67 14]\n",
      " [ 1 12]]\n",
      "ANN Probe Evaluation: Time taken 0.67\n",
      "Results for Fold 2:\n",
      "ann_acc: 0.8404\n",
      "ann_bacc: 0.8751\n",
      "ann_kappa: 0.5284\n",
      "ann_weighted_f1: 0.8601\n",
      "ann_report: {'0': {'precision': 0.9852941176470589, 'recall': 0.8271604938271605, 'f1-score': 0.8993288590604027, 'support': 81.0}, '1': {'precision': 0.46153846153846156, 'recall': 0.9230769230769231, 'f1-score': 0.6153846153846154, 'support': 13.0}, 'accuracy': 0.8404255319148937, 'macro avg': {'precision': 0.7234162895927603, 'recall': 0.8751187084520418, 'f1-score': 0.7573567372225091, 'support': 94.0}, 'weighted avg': {'precision': 0.9128598247809763, 'recall': 0.8404255319148937, 'f1-score': 0.8600599742967301, 'support': 94.0}}\n",
      "ann_auroc: 0.8775\n",
      "Running for Fold 3 as test set\n",
      "Training on 294 samples, Testing on 111 samples\n",
      "ANN Probe Evaluation: Train shape torch.Size([294, 2048])\n",
      "ANN Probe Evaluation: Test shape torch.Size([111, 2048])\n",
      "ANN Probe Evaluation (Train Time): Best cost = 40.960\n",
      "ANN Probe Evaluation (Train Time): Using only train set for evaluation. Train Shape:  torch.Size([294, 2048])\n",
      "(After Training) Loss: 27.613\n",
      "ANN Probe Evaluation (Test Time): Test Shape torch.Size([111, 2048])\n",
      "Confusion Matrix:\n",
      "[[75 16]\n",
      " [ 3 17]]\n",
      "ANN Probe Evaluation: Time taken 0.73\n",
      "Results for Fold 3:\n",
      "ann_acc: 0.8288\n",
      "ann_bacc: 0.8371\n",
      "ann_kappa: 0.5378\n",
      "ann_weighted_f1: 0.8432\n",
      "ann_report: {'0': {'precision': 0.9615384615384616, 'recall': 0.8241758241758241, 'f1-score': 0.8875739644970414, 'support': 91.0}, '1': {'precision': 0.5151515151515151, 'recall': 0.85, 'f1-score': 0.6415094339622641, 'support': 20.0}, 'accuracy': 0.8288288288288288, 'macro avg': {'precision': 0.7383449883449884, 'recall': 0.837087912087912, 'f1-score': 0.7645416992296528, 'support': 111.0}, 'weighted avg': {'precision': 0.8811083811083811, 'recall': 0.8288288288288288, 'f1-score': 0.8432380130493339, 'support': 111.0}}\n",
      "ann_auroc: 0.9225\n",
      "Running for Fold 4 as test set\n",
      "Training on 305 samples, Testing on 100 samples\n",
      "ANN Probe Evaluation: Train shape torch.Size([305, 2048])\n",
      "ANN Probe Evaluation: Test shape torch.Size([100, 2048])\n",
      "ANN Probe Evaluation (Train Time): Best cost = 40.960\n",
      "ANN Probe Evaluation (Train Time): Using only train set for evaluation. Train Shape:  torch.Size([305, 2048])\n",
      "(After Training) Loss: 27.666\n",
      "ANN Probe Evaluation (Test Time): Test Shape torch.Size([100, 2048])\n",
      "Confusion Matrix:\n",
      "[[68 19]\n",
      " [ 4  9]]\n",
      "ANN Probe Evaluation: Time taken 0.70\n",
      "Results for Fold 4:\n",
      "ann_acc: 0.7700\n",
      "ann_bacc: 0.7370\n",
      "ann_kappa: 0.3179\n",
      "ann_weighted_f1: 0.8012\n",
      "ann_report: {'0': {'precision': 0.9444444444444444, 'recall': 0.7816091954022989, 'f1-score': 0.8553459119496856, 'support': 87.0}, '1': {'precision': 0.32142857142857145, 'recall': 0.6923076923076923, 'f1-score': 0.43902439024390244, 'support': 13.0}, 'accuracy': 0.77, 'macro avg': {'precision': 0.6329365079365079, 'recall': 0.7369584438549956, 'f1-score': 0.647185151096794, 'support': 100.0}, 'weighted avg': {'precision': 0.863452380952381, 'recall': 0.77, 'f1-score': 0.8012241141279337, 'support': 100.0}}\n",
      "ann_auroc: 0.8126\n",
      "\n",
      "Average Results across all folds:\n",
      "ann_acc: 0.8123\n",
      "ann_bacc: 0.8206\n",
      "ann_kappa: 0.4641\n",
      "ann_weighted_f1: 0.8342\n",
      "ann_report: {'accuracy': 0.8123135901859306}\n",
      "ann_auroc: 0.8769\n"
     ]
    }
   ],
   "source": [
    "# Function to run cross-validation\n",
    "def run_k_fold_cross_validation():\n",
    "    results_per_fold = []\n",
    "\n",
    "    for i, (test_ids, fold_name) in enumerate(folds):\n",
    "        # Prepare training and validation sets (all folds except the test fold)\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for j, (train_ids, _) in enumerate(folds):\n",
    "            if i == j:\n",
    "                continue  # Skip the test fold\n",
    "            fold_features, fold_labels = load_features_and_labels(save_dir, train_ids)\n",
    "            train_features.append(fold_features)\n",
    "            train_labels.append(fold_labels)\n",
    "\n",
    "        # Concatenate training features and labels from all training folds\n",
    "        train_features = torch.cat(train_features)\n",
    "        train_labels = torch.cat(train_labels)\n",
    "\n",
    "        # Load test data (current fold as the test set)\n",
    "        test_features, test_labels = load_features_and_labels(save_dir, test_ids)\n",
    "\n",
    "        print(f\"Running for {fold_name} as test set\")\n",
    "        print(f\"Training on {train_features.shape[0]} samples, Testing on {test_features.shape[0]} samples\")\n",
    "\n",
    "        # Train and evaluate the model on the current fold\n",
    "        from uni.downstream.eval_patch_features.ann import eval_ANN_probe\n",
    "\n",
    "        linprobe_eval_metrics, linprobe_dump = eval_ANN_probe(\n",
    "            train_feats = train_features,\n",
    "            train_labels = train_labels,\n",
    "            valid_feats =None ,\n",
    "            valid_labels = None,\n",
    "            test_feats = test_features,\n",
    "            test_labels =test_labels,\n",
    "            input_dim=2048,\n",
    "            max_iter = 120,\n",
    "            verbose= True,\n",
    "        )\n",
    "\n",
    "        print(f\"Results for {fold_name}:\")\n",
    "        print_metrics(linprobe_eval_metrics)\n",
    "        results_per_fold.append(linprobe_eval_metrics)\n",
    "\n",
    "    return results_per_fold\n",
    "\n",
    "# Run k-fold cross-validation\n",
    "all_fold_results = run_k_fold_cross_validation()\n",
    "average_results = average_metrics(all_fold_results)\n",
    "print(\"\\nAverage Results across all folds:\")\n",
    "print_metrics(average_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN and ProtoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for Fold 1 as test set\n",
      "Training on 305 samples, Testing on 100 samples\n",
      "Results for Fold 1:\n",
      "knn5_acc: 0.8400\n",
      "knn5_bacc: 0.5490\n",
      "knn5_kappa: 0.1351\n",
      "knn5_weighted_f1: 0.8044\n",
      "knn5_report: {'0': {'precision': 0.8631578947368421, 'recall': 0.9647058823529412, 'f1-score': 0.9111111111111111, 'support': 85.0}, '1': {'precision': 0.4, 'recall': 0.13333333333333333, 'f1-score': 0.2, 'support': 15.0}, 'accuracy': 0.84, 'macro avg': {'precision': 0.631578947368421, 'recall': 0.5490196078431373, 'f1-score': 0.5555555555555556, 'support': 100.0}, 'weighted avg': {'precision': 0.7936842105263158, 'recall': 0.84, 'f1-score': 0.8044444444444444, 'support': 100.0}}\n",
      "proto_acc: 0.8100\n",
      "proto_bacc: 0.7510\n",
      "proto_kappa: 0.4025\n",
      "proto_weighted_f1: 0.8266\n",
      "proto_report: {'0': {'precision': 0.9342105263157895, 'recall': 0.8352941176470589, 'f1-score': 0.8819875776397516, 'support': 85.0}, '1': {'precision': 0.4166666666666667, 'recall': 0.6666666666666666, 'f1-score': 0.5128205128205128, 'support': 15.0}, 'accuracy': 0.81, 'macro avg': {'precision': 0.6754385964912281, 'recall': 0.7509803921568627, 'f1-score': 0.6974040452301322, 'support': 100.0}, 'weighted avg': {'precision': 0.8565789473684211, 'recall': 0.81, 'f1-score': 0.8266125179168659, 'support': 100.0}}\n",
      "Running for Fold 2 as test set\n",
      "Training on 311 samples, Testing on 94 samples\n",
      "Results for Fold 2:\n",
      "knn5_acc: 0.8830\n",
      "knn5_bacc: 0.6415\n",
      "knn5_kappa: 0.3656\n",
      "knn5_weighted_f1: 0.8638\n",
      "knn5_report: {'0': {'precision': 0.8977272727272727, 'recall': 0.9753086419753086, 'f1-score': 0.9349112426035503, 'support': 81.0}, '1': {'precision': 0.6666666666666666, 'recall': 0.3076923076923077, 'f1-score': 0.42105263157894735, 'support': 13.0}, 'accuracy': 0.8829787234042553, 'macro avg': {'precision': 0.7821969696969697, 'recall': 0.6415004748338082, 'f1-score': 0.6779819370912488, 'support': 94.0}, 'weighted avg': {'precision': 0.8657720825274018, 'recall': 0.8829787234042553, 'f1-score': 0.8638456900150414, 'support': 94.0}}\n",
      "proto_acc: 0.8298\n",
      "proto_bacc: 0.7398\n",
      "proto_kappa: 0.4018\n",
      "proto_weighted_f1: 0.8425\n",
      "proto_report: {'0': {'precision': 0.9333333333333333, 'recall': 0.8641975308641975, 'f1-score': 0.8974358974358975, 'support': 81.0}, '1': {'precision': 0.42105263157894735, 'recall': 0.6153846153846154, 'f1-score': 0.5, 'support': 13.0}, 'accuracy': 0.8297872340425532, 'macro avg': {'precision': 0.6771929824561403, 'recall': 0.7397910731244064, 'f1-score': 0.6987179487179487, 'support': 94.0}, 'weighted avg': {'precision': 0.8624860022396416, 'recall': 0.8297872340425532, 'f1-score': 0.8424713584288053, 'support': 94.0}}\n",
      "Running for Fold 3 as test set\n",
      "Training on 294 samples, Testing on 111 samples\n",
      "Results for Fold 3:\n",
      "knn5_acc: 0.8018\n",
      "knn5_bacc: 0.5475\n",
      "knn5_kappa: 0.1241\n",
      "knn5_weighted_f1: 0.7655\n",
      "knn5_report: {'0': {'precision': 0.8349514563106796, 'recall': 0.945054945054945, 'f1-score': 0.8865979381443299, 'support': 91.0}, '1': {'precision': 0.375, 'recall': 0.15, 'f1-score': 0.21428571428571427, 'support': 20.0}, 'accuracy': 0.8018018018018018, 'macro avg': {'precision': 0.6049757281553398, 'recall': 0.5475274725274725, 'f1-score': 0.550441826215022, 'support': 111.0}, 'weighted avg': {'precision': 0.752077320038485, 'recall': 0.8018018018018018, 'f1-score': 0.7654606005121469, 'support': 111.0}}\n",
      "proto_acc: 0.8108\n",
      "proto_bacc: 0.7871\n",
      "proto_kappa: 0.4727\n",
      "proto_weighted_f1: 0.8251\n",
      "proto_report: {'0': {'precision': 0.9375, 'recall': 0.8241758241758241, 'f1-score': 0.8771929824561403, 'support': 91.0}, '1': {'precision': 0.4838709677419355, 'recall': 0.75, 'f1-score': 0.5882352941176471, 'support': 20.0}, 'accuracy': 0.8108108108108109, 'macro avg': {'precision': 0.7106854838709677, 'recall': 0.7870879120879121, 'f1-score': 0.7327141382868937, 'support': 111.0}, 'weighted avg': {'precision': 0.8557650392327811, 'recall': 0.8108108108108109, 'f1-score': 0.8251285341068623, 'support': 111.0}}\n",
      "Running for Fold 4 as test set\n",
      "Training on 305 samples, Testing on 100 samples\n",
      "Results for Fold 4:\n",
      "knn5_acc: 0.8900\n",
      "knn5_bacc: 0.6424\n",
      "knn5_kappa: 0.3693\n",
      "knn5_weighted_f1: 0.8719\n",
      "knn5_report: {'0': {'precision': 0.9042553191489362, 'recall': 0.9770114942528736, 'f1-score': 0.9392265193370166, 'support': 87.0}, '1': {'precision': 0.6666666666666666, 'recall': 0.3076923076923077, 'f1-score': 0.42105263157894735, 'support': 13.0}, 'accuracy': 0.89, 'macro avg': {'precision': 0.7854609929078014, 'recall': 0.6423519009725907, 'f1-score': 0.680139575457982, 'support': 100.0}, 'weighted avg': {'precision': 0.8733687943262411, 'recall': 0.89, 'f1-score': 0.8718639139284676, 'support': 100.0}}\n",
      "proto_acc: 0.8100\n",
      "proto_bacc: 0.7272\n",
      "proto_kappa: 0.3511\n",
      "proto_weighted_f1: 0.8292\n",
      "proto_report: {'0': {'precision': 0.9358974358974359, 'recall': 0.8390804597701149, 'f1-score': 0.8848484848484849, 'support': 87.0}, '1': {'precision': 0.36363636363636365, 'recall': 0.6153846153846154, 'f1-score': 0.45714285714285713, 'support': 13.0}, 'accuracy': 0.81, 'macro avg': {'precision': 0.6497668997668997, 'recall': 0.7272325375773652, 'f1-score': 0.670995670995671, 'support': 100.0}, 'weighted avg': {'precision': 0.8615034965034966, 'recall': 0.81, 'f1-score': 0.8292467532467532, 'support': 100.0}}\n",
      "\n",
      "KNN Average Results across all folds:\n",
      "knn5_acc: 0.8537\n",
      "knn5_bacc: 0.5951\n",
      "knn5_kappa: 0.2485\n",
      "knn5_weighted_f1: 0.8264\n",
      "knn5_report: {'accuracy': 0.8536951313015143}\n",
      "\n",
      "ProtoNet Average Results across all folds:\n",
      "proto_acc: 0.8151\n",
      "proto_bacc: 0.7513\n",
      "proto_kappa: 0.4070\n",
      "proto_weighted_f1: 0.8309\n",
      "proto_report: {'accuracy': 0.815149511213341}\n"
     ]
    }
   ],
   "source": [
    "# Function to run cross-validation\n",
    "def run_k_fold_cross_validation():\n",
    "    knn_results_per_fold = []\n",
    "    protonet_results_per_fold = []\n",
    "\n",
    "    for i, (test_ids, fold_name) in enumerate(folds):\n",
    "        # Prepare training and validation sets (all folds except the test fold)\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for j, (train_ids, _) in enumerate(folds):\n",
    "            if i == j:\n",
    "                continue  # Skip the test fold\n",
    "            fold_features, fold_labels = load_features_and_labels(save_dir, train_ids)\n",
    "            train_features.append(fold_features)\n",
    "            train_labels.append(fold_labels)\n",
    "\n",
    "        # Concatenate training features and labels from all training folds\n",
    "        train_features = torch.cat(train_features)\n",
    "        train_labels = torch.cat(train_labels)\n",
    "\n",
    "        # Load test data (current fold as the test set)\n",
    "        test_features, test_labels = load_features_and_labels(save_dir, test_ids)\n",
    "\n",
    "        print(f\"Running for {fold_name} as test set\")\n",
    "        print(f\"Training on {train_features.shape[0]} samples, Testing on {test_features.shape[0]} samples\")\n",
    "\n",
    "        from uni.downstream.eval_patch_features.fewshot import eval_knn\n",
    "\n",
    "        knn_eval_metrics, knn_dump, proto_eval_metrics, proto_dump = eval_knn(\n",
    "            train_feats = train_features,\n",
    "            train_labels =train_labels,\n",
    "            test_feats = test_features,\n",
    "            test_labels =test_labels,\n",
    "            center_feats = True,\n",
    "            normalize_feats = True,\n",
    "            n_neighbors = 5\n",
    "        )\n",
    "\n",
    "        print(f\"Results for {fold_name}:\")\n",
    "        print_metrics(knn_eval_metrics)\n",
    "        print_metrics(proto_eval_metrics)\n",
    "        knn_results_per_fold.append(knn_eval_metrics)\n",
    "        protonet_results_per_fold.append(proto_eval_metrics)\n",
    "\n",
    "    return knn_results_per_fold, protonet_results_per_fold\n",
    "\n",
    "# Run k-fold cross-validation\n",
    "knn_all_fold_results, protonet_all_fold_results = run_k_fold_cross_validation()\n",
    "knn_average_results = average_metrics(knn_all_fold_results)\n",
    "proto_average_results = average_metrics(protonet_all_fold_results)\n",
    "print(\"\\nKNN Average Results across all folds:\")\n",
    "print_metrics(knn_average_results)\n",
    "print(\"\\nProtoNet Average Results across all folds:\")\n",
    "print_metrics(proto_average_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
