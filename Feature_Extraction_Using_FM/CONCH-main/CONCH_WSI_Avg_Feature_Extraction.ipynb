{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import sys\n",
    "from os.path import join as j_\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# print(torch.version)\n",
    "# print(torch.version.cuda)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from torchvision.transforms import Lambda\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "from conch.open_clip_custom import create_model_from_pretrained, get_tokenizer, tokenize\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "BATCH_SIZE = 1 # load each slide all tiles sequentially \n",
    "K_FOLDS_PATH = r\"E:\\KSA Project\\\\dataset\\\\splits\\kfolds_IDARS.csv\"\n",
    "DATA_PATH = r\"E:\\\\KSA Project\\\\dataset\\\\testing\\\\Patches\"\n",
    "FEATURES_SAVE_DIR = r\"E:\\\\KSA Project\\\\dataset\\\\testing\\\\conch_features\"\n",
    "# torch.tensor([1.2, 3.4]).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model \"create_model_from_pretrained\"\n",
    "By default, the model preprocessor uses 448 x 448 as the input size. To specify a different image size (e.g. 336 x 336), use the **force_img_size** argument.\n",
    "\n",
    "You can specify a cuda device by using the **device** argument, or manually move the model to a device later using **model.to(device)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = 'conch_ViT-B-16'\n",
    "checkpoint_path = './checkpoints/pytorch_model.bin'\n",
    "model, preprocess = create_model_from_pretrained(model_cfg, checkpoint_path, device=device)\n",
    "# model, preprocess = create_model_from_pretrained(model_cfg, checkpoint_path, force_img_size=224, device='cuda:2')\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import PatchLoader, SlideBatchSampler\n",
    "mode = 1 # for sequentially data/patches loading we will use mode =1 and mode= 2 for random loading.\n",
    "transform = transforms.Compose([\n",
    "    transforms.FiveCrop(224),  # this is a list of 5 crops\n",
    "    Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))  # convert to tensor and stack\n",
    "])\n",
    "\n",
    "def create_dataloader(label_file, data_path, transform, num_samples, mode):\n",
    "    dataset = PatchLoader(label_file=label_file, data_path=data_path, transform=transform, num_samples=num_samples, mode=mode)\n",
    "    batch_sampler = SlideBatchSampler(dataset.ntiles)\n",
    "    dataloader = DataLoader(dataset, batch_sampler=batch_sampler, num_workers=0, pin_memory=False)\n",
    "    return dataloader\n",
    "\n",
    "# Create DataLoaders\n",
    "data_loader = create_dataloader(label_file=K_FOLDS_PATH,data_path=DATA_PATH,\n",
    "                                     transform=transform,num_samples=None,mode=mode)\n",
    "print(f\"Length of data_loader: {len(data_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Images shape: torch.Size([5, 3, 224, 224])\n",
      "Labels: tensor([0, 0, 0, 0, 0])\n",
      "Batch 2\n",
      "Images shape: torch.Size([5, 3, 224, 224])\n",
      "Labels: tensor([0, 0, 0, 0, 0, 0])\n",
      "Batch 3\n",
      "Images shape: torch.Size([5, 3, 224, 224])\n",
      "Labels: tensor([1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "    print(f\"Batch {batch_idx+1}\")\n",
    "    print(f\"Images shape: {images.shape}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    if batch_idx == 2:  # Only print a few batches to check if it's working\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Feature Extraction on FiveCrop Patches Using Averaging Approach at WSI Level\n",
    "Extract patches of individual patch five crops then average and at the end average features of all patches of each WSI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_embeddings_with_chunks(model, preprocess, dataloader, chunk_size=1000):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    device = next(model.parameters()).device\n",
    "    # print(f'The size of input dataloader is {len(dataloader)}')\n",
    "\n",
    "    for batch_idx, (images, labels) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        batch_size, num_crops, channels, height, width = images.shape\n",
    "        images = images.view(batch_size * num_crops, channels, height, width)\n",
    "        images = images.permute(0, 2, 3, 1).cpu().numpy()  # Convert to (H, W, C) format\n",
    "        images = (images * 255).astype(np.uint8)  # Convert to uint8\n",
    "\n",
    "        # Preprocess images\n",
    "        images = torch.stack([preprocess(Image.fromarray(image)) for image in images])\n",
    "        images = images.to(device)\n",
    "\n",
    "        wsi_embeddings = []\n",
    "        num_patches = images.size(0)\n",
    "        num_chunks = (num_patches + chunk_size - 1) // chunk_size\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min(start_idx + chunk_size, num_patches)\n",
    "            chunk = images[start_idx:end_idx]\n",
    "            with torch.inference_mode(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                # Extract embeddings for the chunk\n",
    "                chunk_embeddings = model.encode_image(chunk).detach().cpu()\n",
    "                chunk_embeddings = chunk_embeddings.view(-1, num_crops, chunk_embeddings.size(-1))  # Reshape to [chunk_size, num_crops, embedding_dim]\n",
    "                chunk_embeddings = chunk_embeddings.mean(dim=1)  # Average over five-crop features\n",
    "                wsi_embeddings.append(chunk_embeddings)\n",
    "        # Concatenate embeddings for all chunks in this WSI\n",
    "        wsi_embeddings = torch.cat(wsi_embeddings, dim=0)  # [num_patches, embedding_dim]\n",
    "        # print(f'WSI embeddings shape: {wsi_embeddings.shape}')\n",
    "        slide_embedding = wsi_embeddings.mean(dim=0)  # Average over all patches\n",
    "        wsi_label = labels[0].item()  # Assuming all labels in the batch are the same\n",
    "        all_embeddings.append(slide_embedding.numpy())\n",
    "        all_labels.append(wsi_label)\n",
    "        # print (f'Batch {batch_idx} done')\n",
    "        # print(f'shape of all embeddings: {len(all_embeddings)}')\n",
    "\n",
    "    # Stack the embeddings and labels\n",
    "    asset_dict = {\n",
    "        \"embeddings\": np.stack(all_embeddings).astype(np.float32),  # [num_slides, embedding_dim]\n",
    "        \"labels\": np.array(all_labels),\n",
    "    }\n",
    "\n",
    "    return asset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "features = extract_embeddings_with_chunks(model, preprocess, data_loader)\n",
    "\n",
    "# convert these to torch\n",
    "feats = torch.Tensor(features['embeddings'])\n",
    "labels = torch.Tensor(features['labels']).type(torch.long)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f'Took {elapsed:.03f} seconds')\n",
    "print(f'Train features shape {feats.shape} and Labels shape is {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(features, dataloader, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    slides = dataloader.dataset.slides\n",
    "    for i, slide in enumerate(slides):\n",
    "        slide_feats = features[i]\n",
    "        slide_name = slide.split('.')[0]\n",
    "        # save as torch .pt file \n",
    "        save_path = os.path.join(save_dir, f'{slide_name}.pt')\n",
    "        torch.save(slide_feats, save_path)\n",
    "        # print(f'Saved features for slide {slide_name} to {save_path}')\n",
    "\n",
    "save_features(feats, data_loader, FEATURES_SAVE_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
