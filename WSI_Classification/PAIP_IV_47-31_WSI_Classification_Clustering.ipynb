{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Instruction before you run this notebook.\n",
    "1. Change the line 29 of dataloader_clustering module from wsi_id = wsi_file[:12] to wsi_id = wsi_folder\n",
    "2. Edit the second cell configurations according to your paths\n",
    "3. Use patch level features for this. (Fivecrops or Patch Level Averaged) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "from os.path import join as j_\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "# print(torch.version)\n",
    "# print(torch.version.cuda)\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Lambda\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "# loading all packages here to start\n",
    "from dataloader_clustering import WSIDataset\n",
    "from eval_patch_features.logistic import eval_linear\n",
    "from eval_patch_features.ann import eval_ANN\n",
    "from eval_patch_features.knn import eval_knn\n",
    "from eval_patch_features.protonet import eval_protonet\n",
    "from eval_patch_features.metrics import get_eval_metrics, print_metrics\n",
    "from utility import calculate_metric_averages, average_confusion_matrices, write_data_in_excel, build_probs_df\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "VECTOR_DIM = 1280\n",
    "HIDDEN_DIM = 768 \n",
    "FM_MODEL = \"baseline\"\n",
    "RUNS_RESULT = \"average\"\n",
    "ANN_RUNS = 20\n",
    "CLUSTERING_METHOD = 'kmeans'\n",
    "NUM_CLUSTERS = 2\n",
    "NUM_PATCHES_PER_CLUSTER = 0\n",
    "BATCH_SIZE = 4\n",
    "K_FOLDS_PATH = r\"E:\\Aamir Gulzar\\dataset\\paip_data\\labels\\TrainTest_paip.csv\"\n",
    "DATA_PATH = f\"E:\\Aamir Gulzar\\dataset\\paip_data\\{FM_MODEL}_FiveCrop_Features\"\n",
    "MODEL_SAVE_PATH = f\"E:\\KSA Project\\KSAproject_pipeline1\\WSI_Classification\\Clustering\\PAIP-IV\\{FM_MODEL}_{NUM_CLUSTERS}Cluster_Classifiers\"\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "OUTPUT_SAVE_PATH = r\"E:\\KSA Project\\KSAproject_pipeline1\\WSI_Classification\\Clustering\\PAIP-IV\"\n",
    "os.makedirs(OUTPUT_SAVE_PATH, exist_ok=True)\n",
    "# create a excel sheet in the output folder to save the results\n",
    "EVAL_METRICS_EXCEL = os.path.join(OUTPUT_SAVE_PATH, \"PAIP-IV_2cluster_eval_metrics.xlsx\")\n",
    "PROBS_ALL_EXCEL = os.path.join(OUTPUT_SAVE_PATH, \"PAIP-IV_2cluster_probs_all.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(fold,train_loader, test_loader, model_type='lin'):\n",
    "    all_train_feats, all_train_labels, all_test_feats, all_test_labels = [], [], [], []\n",
    "    all_test_ids = []\n",
    "    # Prepare training and testing data\n",
    "    for features, label, _ in train_loader:\n",
    "        all_train_feats.append(features)\n",
    "        all_train_labels.append(label)\n",
    "    for features, label, wsi_id in test_loader:\n",
    "        all_test_feats.append(features)\n",
    "        all_test_labels.append(label)\n",
    "        # Store as single WSI IDs from the batch \n",
    "        if isinstance(wsi_id, (list, tuple)):\n",
    "            all_test_ids.extend(wsi_id)\n",
    "        else:\n",
    "            all_test_ids.append(wsi_id)\n",
    "    # Convert lists to tensors\n",
    "    global train_feats, train_labels, val_feats, val_labels, test_feats, test_labels\n",
    "    train_feats = torch.cat(all_train_feats)\n",
    "    train_labels = torch.cat([labels.clone().detach() for labels in all_train_labels])\n",
    "    test_feats = torch.cat(all_test_feats)\n",
    "    test_labels = torch.cat([labels.clone().detach() for labels in all_test_labels])\n",
    "    \n",
    "    # Select the model based on the input argument\n",
    "    if model_type == 'lin':\n",
    "        eval_metrics, eval_dump = eval_linear(\n",
    "            fold=fold,\n",
    "            train_feats=train_feats,\n",
    "            train_labels=train_labels,\n",
    "            valid_feats=None,  # Optionally, use a separate validation set\n",
    "            valid_labels=None,\n",
    "            test_feats=test_feats,\n",
    "            test_labels=test_labels,\n",
    "            max_iter=350,\n",
    "            save_path = MODEL_SAVE_PATH,\n",
    "            verbose=False,\n",
    "        )\n",
    "    elif model_type == 'ann':\n",
    "        eval_metrics, eval_dump = eval_ANN(\n",
    "            fold=fold,\n",
    "            train_feats=train_feats,\n",
    "            train_labels=train_labels,\n",
    "            valid_feats=None,\n",
    "            valid_labels=None,\n",
    "            test_feats=test_feats,\n",
    "            test_labels=test_labels,\n",
    "            input_dim=VECTOR_DIM * NUM_CLUSTERS,\n",
    "            hidden_dim = HIDDEN_DIM,\n",
    "            model_save_path = MODEL_SAVE_PATH,\n",
    "            max_iter=350,\n",
    "            num_runs=ANN_RUNS,  # Run the function 5 times\n",
    "            runs_results=RUNS_RESULT,  # Choose \"average\" or \"best\"\n",
    "            metric_weights={\"bacc\": 0.5, \"auroc\": 0.5},  # Prioritize balanced accuracy\n",
    "            verbose=False,\n",
    "        )\n",
    "    elif model_type == 'knn':\n",
    "        eval_metrics, eval_dump = eval_knn(\n",
    "            fold=fold,\n",
    "            train_feats=train_feats,\n",
    "            train_labels=train_labels,\n",
    "            val_feats=None,\n",
    "            val_labels=None,\n",
    "            test_feats=test_feats,\n",
    "            test_labels=test_labels,\n",
    "            n_neighbors=5,\n",
    "            normalize_feats=True,\n",
    "            model_save_path = MODEL_SAVE_PATH,\n",
    "            verbose=False\n",
    "        )\n",
    "    elif model_type == 'proto':\n",
    "        eval_metrics, eval_dump = eval_protonet(\n",
    "            fold=fold,\n",
    "            train_feats=train_feats,\n",
    "            train_labels=train_labels,\n",
    "            val_feats=None,\n",
    "            val_labels=None,\n",
    "            test_feats=test_feats,\n",
    "            test_labels=test_labels,\n",
    "            normalize_feats=True,\n",
    "            model_save_path = MODEL_SAVE_PATH\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "    return eval_metrics, eval_dump, all_test_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "def count_classes(dataset):\n",
    "    \"\"\"\n",
    "    Helper function to count class occurrences in a dataset.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for _, label, _ in DataLoader(dataset, batch_size=1, shuffle=False):\n",
    "        labels.append(label.item() if isinstance(label, torch.Tensor) else label)\n",
    "    return Counter(labels)\n",
    "\n",
    "# Cross-validation function\n",
    "def run_k_fold_cross_validation(save_dir: str, folds: List[List[str]], model_type: str = 'linear'):\n",
    "    results_per_fold = []\n",
    "\n",
    "    num_folds = len(folds)\n",
    "\n",
    "    for i in range(1):\n",
    "        # Define test and validation folds\n",
    "        train_ids = folds[i]\n",
    "        test_ids = folds[i + 1]  # The next fold in sequence is used as validation\n",
    "\n",
    "        # Create datasets and loaders\n",
    "        train_dataset = WSIDataset(save_dir, train_ids)\n",
    "        train_dataset.apply_clustering(clustering_algorithm=CLUSTERING_METHOD, num_clusters=NUM_CLUSTERS, num_selected_patches=NUM_PATCHES_PER_CLUSTER)\n",
    "        test_dataset = WSIDataset(save_dir, test_ids)\n",
    "        test_dataset.apply_clustering(clustering_algorithm=CLUSTERING_METHOD,num_clusters=NUM_CLUSTERS, num_selected_patches=NUM_PATCHES_PER_CLUSTER)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        # Train and evaluate\n",
    "        print(f\"Running Fold {i + 1} with model {model_type}...\")\n",
    "        eval_metrics, eval_dump, all_test_ids = train_and_evaluate(i,train_loader,test_loader, model_type=model_type)\n",
    "        print_metrics(eval_metrics)\n",
    "        result = {\n",
    "            **eval_metrics,\n",
    "            **eval_dump,\n",
    "            \"wsi_ids\":all_test_ids ,  # You already have this in train_and_evaluate\n",
    "            \"fold\": i + 1\n",
    "        }\n",
    "        results_per_fold.append(result)\n",
    "\n",
    "    return results_per_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Runner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ********* Training with model: linear********* \n",
      "\n",
      "\n",
      "Running Fold 1 with model linear...\n",
      "lin_acc: 0.6774\n",
      "lin_bacc: 0.5387\n",
      "lin_macro_f1: 0.5387\n",
      "lin_weighted_f1: 0.6774\n",
      "lin_auroc: 0.5595\n",
      "lin_conf_matrix: [[19  5]\n",
      " [ 5  2]]\n",
      "\n",
      "\n",
      " ********* Training with model: ann********* \n",
      "\n",
      "\n",
      "Running Fold 1 with model ann...\n",
      "acc: 0.6129\n",
      "bacc: 0.6488\n",
      "macro_f1: 0.5773\n",
      "weighted_f1: 0.6446\n",
      "auroc: 0.6190\n",
      "conf_matrix: [[14 10]\n",
      " [ 2  5]]\n",
      "\n",
      "\n",
      " ********* Training with model: knn********* \n",
      "\n",
      "\n",
      "Running Fold 1 with model knn...\n",
      "knn_acc: 0.4839\n",
      "knn_bacc: 0.4137\n",
      "knn_macro_f1: 0.4095\n",
      "knn_weighted_f1: 0.5244\n",
      "knn_auroc: 0.4643\n",
      "knn_conf_matrix: [[13 11]\n",
      " [ 5  2]]\n",
      "\n",
      "\n",
      " ********* Training with model: protonet********* \n",
      "\n",
      "\n",
      "Running Fold 1 with model protonet...\n",
      "proto_acc: 0.6774\n",
      "proto_bacc: 0.5387\n",
      "proto_macro_f1: 0.5387\n",
      "proto_weighted_f1: 0.6774\n",
      "proto_auroc: 0.7262\n",
      "proto_conf_matrix: [[19  5]\n",
      " [ 5  2]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "folds_df = pd.read_csv(K_FOLDS_PATH)\n",
    "# Define your folds\n",
    "fold1_ids = folds_df['Fold1'].dropna().tolist()\n",
    "fold2_ids = folds_df['Fold2'].dropna().tolist()\n",
    "folds = [fold1_ids, fold2_ids]\n",
    "# Run k-fold cross-validation with different models\n",
    "model_types = ['lin','ann','knn','proto']\n",
    "# model_types = ['ann','protonet']\n",
    "metric_indices = {\n",
    "    'acc': 0,          # 'lin_acc' corresponds to index 0\n",
    "    'bacc': 1,         # 'lin_bacc' corresponds to index 1\n",
    "    'macro_f1': 2,     # 'lin_macro_f1' corresponds to index 2\n",
    "    'weighted_f1': 3,  # 'lin_weighted_f1' corresponds to index 3\n",
    "    'auroc': 4         # 'lin_auroc' corresponds to index 4\n",
    "}\n",
    "\n",
    "eval_metrics__for_excel = []\n",
    "probs_all_for_excel = None\n",
    "for model in model_types:\n",
    "    print(f\"\\n\\n ********* Training with model: {model}********* \\n\\n\")\n",
    "    k_folds_results = run_k_fold_cross_validation(DATA_PATH, folds, model_type=model)\n",
    "    model_df = build_probs_df(k_folds_results,model_name=model)\n",
    "    # === Merge predictions across models ===\n",
    "    if probs_all_for_excel is None:\n",
    "        probs_all_for_excel = model_df\n",
    "    else:\n",
    "        probs_all_for_excel=pd.merge(probs_all_for_excel,model_df,on=[\"Fold\", \"WSI_ID\", \"Target\"],how=\"outer\")\n",
    "\n",
    "    # === Average metrics (only pass metric parts of result dicts)\n",
    "    average_results = calculate_metric_averages(\n",
    "        [{k: v for k, v in result.items() if k in [f\"{model}_{m}\" for m in metric_indices.keys()]}\n",
    "        for result in k_folds_results],\n",
    "        metric_indices,\n",
    "        model_prefix=model\n",
    "    )\n",
    "    # === Confusion matrices\n",
    "    confusion_matrices = [np.array(result[f\"{model}_conf_matrix\"]) for result in k_folds_results if f\"{model}_conf_matrix\" in result]\n",
    "    \n",
    "    avg_conf_matrix = average_confusion_matrices(confusion_matrices)\n",
    "    print(\"\\n\\n Average results for all folds:\")\n",
    "    for metric, value in average_results.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "     # Append per metric rows for each fold + average\n",
    "    for metric in metric_indices.keys():\n",
    "        row = [f\"{model}_{metric}\"]\n",
    "        for result in k_folds_results:\n",
    "            row.append(result.get(f\"{model}_{metric}\", 'N/A'))\n",
    "        row.append(average_results.get(f\"{model}_{metric}\", 'N/A'))\n",
    "        eval_metrics__for_excel.append(row)\n",
    "\n",
    "    # Append confusion matrix as string (per fold)\n",
    "    row = [f\"{model}_conf_matrix\"]\n",
    "    for result in k_folds_results:\n",
    "        row.append(str(result.get(f\"{model}_conf_matrix\", \"N/A\")))\n",
    "    row.append(str(avg_conf_matrix))\n",
    "    eval_metrics__for_excel.append(row)\n",
    "    \n",
    "eval_metrics_df = pd.DataFrame(eval_metrics__for_excel, \n",
    "                        columns=[\"Metric\",\"Fold1\",\"AvgFolds\"])\n",
    "write_data_in_excel(EVAL_METRICS_EXCEL, eval_metrics_df, FM_MODEL)\n",
    "write_data_in_excel(PROBS_ALL_EXCEL, probs_all_for_excel, FM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
